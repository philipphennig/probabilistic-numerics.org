@inproceedings{HennigKiefel,
  author =	 {P. Hennig and M. Kiefel},
  booktitle =	 {{International Conference on Machine Learning (ICML)}},
  title =	 {{Quasi-{N}ewton methods -- a new direction}},
  year =	 2012,
  abstract =	 {Four decades after their invention, quasi- Newton methods are
                  still state of the art in unconstrained numerical
                  optimization. Al- though not usually interpreted thus, these
                  are learning algorithms that fit a local quadratic
                  approximation to the objective function. We show that many,
                  including the most popular, quasi-Newton methods can be
                  interpreted as approximations of Bayesian linear regression
                  under varying prior assumptions. This new notion elucidates
                  some shortcomings of clas- sical algorithms, and lights the
                  way to a novel nonparametric quasi-Newton method, which is
                  able to make more efficient use of available information at
                  computational cost similar to its predecessors.},
  video =
                  {http://techtalks.tv/talks/quasi-newton-methods-a-new-direction/57289/},
  file =	 {../assets/pdf/hennig13quasiNewton.pdf}
}

@article{hennig13_quasi_newton_method,
  author =	 {P. Hennig and M. Kiefel},
  journal =	 {Journal of Machine Learning Research},
  month =	 {March},
  pages =	 {834--865},
  title =	 {Quasi-{N}ewton Methods -- a new direction},
  volume =	 14,
  year =	 2013,
  abstract =	 {Four decades after their invention, quasi-Newton methods are
                  still state of the art in unconstrained numerical
                  optimization. Although not usually interpreted thus, these
                  are learning algorithms that fit a local quadratic
                  approximation to the objective function. We show that many,
                  including the most popular, quasi-Newton methods can be
                  interpreted as approximations of Bayesian linear regression
                  under varying prior assumptions. This new notion elucidates
                  some shortcomings of classical algorithms, and lights the
                  way to a novel nonparametric quasi-Newton method, which is
                  able to make more efficient use of available information at
                  computational cost similar to its predecessors.},
  file = {http://jmlr.org/papers/volume14/hennig13a/hennig13a.pdf}
}

@inproceedings{StochasticNewton,
  author =	 {P. Hennig},
  booktitle =	 {{International Conference on Machine Learning (ICML)}},
  title =	 {{Fast Probabilistic Optimization from Noisy Gradients}},
  year =	 {2013},
  abstract =	 {Stochastic gradient descent remains popular in large-scale
                  machine learning, on account of its very low computational
                  cost and robust- ness to noise. However, gradient descent is
                  only linearly efficient and not transformation
                  invariant. Scaling by a local measure can substantially
                  improve its performance. One natural choice of such a scale
                  is the Hessian of the objective function: Were it available,
                  it would turn linearly efficient gradient descent into the
                  quadratically efficient Newton-Raphson optimization. Existing
                  covariant methods, though, are either super-linearly
                  expensive or do not address noise. Generalising recent
                  results, this paper constructs a nonparametric Bayesian
                  quasi-Newton algorithm that learns gradient and Hessian from
                  noisy evaluations of the gradient. Importantly, the resulting
                  algorithm, like stochastic gradient descent, has cost linear
                  in the number of input dimensions},
  file = {../assets/pdf/hennig13noisy.pdf}
}

@incollection{NIPS2015_5753,
title = {Probabilistic Line Searches for Stochastic Optimization},
author = {Mahsereci, Maren and Hennig, Philipp},
booktitle = {Advances in Neural Information Processing Systems 28},
editor = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
pages = {181--189},
year = {2015},
publisher = {Curran Associates, Inc.},
file = {http://papers.nips.cc/paper/5753-probabilistic-line-searches-for-stochastic-optimization.pdf},
abstract = {In deterministic optimization, line searches are a standard tool ensuring stability and efficiency. Where only stochastic gradients are available, no direct equivalent has so far been formulated, because uncertain gradients do not allow for a strict sequence of decisions collapsing the search space. We construct a probabilistic line search by combining the structure of existing deterministic methods with notions from Bayesian optimization. Our method retains a Gaussian process surrogate of the univariate optimization objective, and uses a probabilistic belief over the Wolfe conditions to monitor the descent. The algorithm has very low computational cost, and no user-controlled parameters. Experiments show that it effectively removes the need to define a learning rate for stochastic gradient descent.},
code = {https://is.tuebingen.mpg.de/uploads_file/attachment/attachment/242/probLS.zip},
link = {http://papers.nips.cc/paper/5753-probabilistic-line-searches-for-stochastic-optimization}
}

@article{DBLP:journals/corr/MasegosaMRCSNLM17,
  author    = {Andr{\'{e}}s R. Masegosa and
               Ana M. Mart{\'{\i}}nez and
               Dar{\'{\i}}o Ramos{-}L{\'{o}}pez and
               Rafael Caba{\~{n}}as and
               Antonio Salmer{\'{o}}n and
               Thomas D. Nielsen and
               Helge Langseth and
               Anders L. Madsen},
  title     = {{AMIDST:} a Java Toolbox for Scalable Probabilistic Machine Learning},
  journal   = {CoRR},
  volume    = {abs/1704.01427},
  year      = {2017},
  abstract  = {The AMIDST Toolbox is a software for scalable probabilistic machine learning with a special focus on (massive) streaming data. The toolbox supports a flexible modeling language based on probabilistic graphical models with latent variables and temporal dependencies. The specified models can be learnt from large data sets using parallel or distributed implementations of Bayesian learning algorithms for either streaming or batch data. These algorithms are based on a flexible variational message passing scheme, which supports discrete and continuous variables from a wide range of probability distributions. AMIDST also leverages existing functionality and algorithms by interfacing to software tools such as Flink, Spark, MOA, Weka, R and HUGIN. AMIDST is an open source toolbox written in Java and available at http://www.amidsttoolbox.com under the Apache Software License version 2.0.},
  code      = {https://github.com/amidst/toolbox},
  url       = {http://arxiv.org/abs/1704.01427},
  timestamp = {Wed, 07 Jun 2017 14:40:25 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/MasegosaMRCSNLM17},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}
