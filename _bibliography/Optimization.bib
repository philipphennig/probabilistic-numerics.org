@inproceedings{HennigKiefel,
  author =	 {P. Hennig and M. Kiefel},
  booktitle =	 {{International Conference on Machine Learning (ICML)}},
  title =	 {{Quasi-{N}ewton methods -- a new direction}},
  year =	 2012,
  abstract =	 {Four decades after their invention, quasi- Newton methods are
                  still state of the art in unconstrained numerical
                  optimization. Al- though not usually interpreted thus, these
                  are learning algorithms that fit a local quadratic
                  approximation to the objective function. We show that many,
                  including the most popular, quasi-Newton methods can be
                  interpreted as approximations of Bayesian linear regression
                  under varying prior assumptions. This new notion elucidates
                  some shortcomings of clas- sical algorithms, and lights the
                  way to a novel nonparametric quasi-Newton method, which is
                  able to make more efficient use of available information at
                  computational cost similar to its predecessors.},
  video =
                  {http://techtalks.tv/talks/quasi-newton-methods-a-new-direction/57289/},
  file =	 {../assets/pdf/hennig13quasiNewton.pdf}
}

@article{hennig13_quasi_newton_method,
  author =	 {P. Hennig and M. Kiefel},
  journal =	 {Journal of Machine Learning Research},
  month =	 {March},
  pages =	 {834--865},
  title =	 {Quasi-{N}ewton Methods -- a new direction},
  volume =	 14,
  year =	 2013,
  abstract =	 {Four decades after their invention, quasi-Newton methods are
                  still state of the art in unconstrained numerical
                  optimization. Although not usually interpreted thus, these
                  are learning algorithms that fit a local quadratic
                  approximation to the objective function. We show that many,
                  including the most popular, quasi-Newton methods can be
                  interpreted as approximations of Bayesian linear regression
                  under varying prior assumptions. This new notion elucidates
                  some shortcomings of classical algorithms, and lights the
                  way to a novel nonparametric quasi-Newton method, which is
                  able to make more efficient use of available information at
                  computational cost similar to its predecessors.},
  file = {http://jmlr.org/papers/volume14/hennig13a/hennig13a.pdf}
}

@inproceedings{StochasticNewton,
  author =	 {P. Hennig},
  booktitle =	 {{International Conference on Machine Learning (ICML)}},
  title =	 {{Fast Probabilistic Optimization from Noisy Gradients}},
  year =	 {2013},
  abstract =	 {Stochastic gradient descent remains popular in large-scale
                  machine learning, on account of its very low computational
                  cost and robust- ness to noise. However, gradient descent is
                  only linearly efficient and not transformation
                  invariant. Scaling by a local measure can substantially
                  improve its performance. One natural choice of such a scale
                  is the Hessian of the objective function: Were it available,
                  it would turn linearly efficient gradient descent into the
                  quadratically efficient Newton-Raphson optimization. Existing
                  covariant methods, though, are either super-linearly
                  expensive or do not address noise. Generalising recent
                  results, this paper constructs a nonparametric Bayesian
                  quasi-Newton algorithm that learns gradient and Hessian from
                  noisy evaluations of the gradient. Importantly, the resulting
                  algorithm, like stochastic gradient descent, has cost linear
                  in the number of input dimensions},
  file = {../assets/pdf/hennig13noisy.pdf}
}

@ARTICLE{Quartz,
   author = {Z. Qu and P. Richt{\'a}rik and T. Zhang},
    title = {Randomized Dual Coordinate Ascent with Arbitrary Sampling},
  journal = {arXiv:1411.5873},
     year = 2014,
   abstract={ We study the problem of minimizing the average of a large number of smooth convex functions penalized  
              with a strongly convex regularizer. We propose and analyze a novel primal-dual method (Quartz) which at every iteration
               samples and updates a random subset of the dual variables, chosen according to an {\em arbitrary distribution}. 
                In contrast to typical analysis, we directly bound the decrease of the primal-dual error (in expectation), without the need to first analyze the dual error.  Depending on the choice of the sampling, we obtain efficient serial, parallel and distributed variants of the method. In the serial case, our bounds match the best known bounds for SDCA (both with  uniform and importance sampling). With standard mini-batching, our bounds predict initial data-independent speedup as well {as \em additional data-driven speedup} which depends on spectral and sparsity properties of the data. We calculate theoretical speedup factors  and find that they are excellent predictors of actual speedup in practice. Moreover, we illustrate that it is possible to design an efficient {\em mini-batch importance} sampling. The distributed variant of Quartz is the first distributed SDCA-like method with an analysis for non-separable data.},
   file = {http://arxiv.org/abs/1411.5873}
}
